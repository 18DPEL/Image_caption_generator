{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7032204,"sourceType":"datasetVersion","datasetId":4044999}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk \nfrom nltk.corpus import wordnet\n\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:22:33.408863Z","iopub.execute_input":"2023-11-22T18:22:33.409251Z","iopub.status.idle":"2023-11-22T18:22:33.654884Z","shell.execute_reply.started":"2023-11-22T18:22:33.409222Z","shell.execute_reply":"2023-11-22T18:22:33.652659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence=\"your destination is kabar do some workship, Galib some said that never go any where without taking somthing\"\nsentence=sentence.lower()\nsentence","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:25:26.709531Z","iopub.execute_input":"2023-11-22T18:25:26.709944Z","iopub.status.idle":"2023-11-22T18:25:26.717156Z","shell.execute_reply.started":"2023-11-22T18:25:26.709908Z","shell.execute_reply":"2023-11-22T18:25:26.715989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmetization","metadata":{}},{"cell_type":"code","source":"import spacy\n\nnlp=spacy.load('en_core_web_sm')\nnlp","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:33:09.329630Z","iopub.execute_input":"2023-11-22T18:33:09.330061Z","iopub.status.idle":"2023-11-22T18:33:10.747115Z","shell.execute_reply.started":"2023-11-22T18:33:09.330023Z","shell.execute_reply":"2023-11-22T18:33:10.746034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatization(text):\n    text_1=nlp(text)\n    \n    lemmatized_tokens=[token.lemma_ for token in text_1]\n    lemmatized_text=' '.join(lemmatized_tokens)\n    return lemmatized_text\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:41:08.783928Z","iopub.execute_input":"2023-11-22T18:41:08.784354Z","iopub.status.idle":"2023-11-22T18:41:08.790048Z","shell.execute_reply.started":"2023-11-22T18:41:08.784321Z","shell.execute_reply":"2023-11-22T18:41:08.788972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word=\"working, running, something, nothing , weeping\"\n\ntest_sentence=lemmatization(word)\nprint(test_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:42:58.775961Z","iopub.execute_input":"2023-11-22T18:42:58.776356Z","iopub.status.idle":"2023-11-22T18:42:58.791814Z","shell.execute_reply.started":"2023-11-22T18:42:58.776324Z","shell.execute_reply":"2023-11-22T18:42:58.790895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"token_sentence=nltk.word_tokenize(sentence)\nprint(token_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T18:46:11.241252Z","iopub.execute_input":"2023-11-22T18:46:11.241704Z","iopub.status.idle":"2023-11-22T18:46:11.247294Z","shell.execute_reply.started":"2023-11-22T18:46:11.241667Z","shell.execute_reply":"2023-11-22T18:46:11.246533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n# Get a set of English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Remove stopwords\nfiltered_sentence = [word for word in token_sentence if word.lower() not in stop_words]\n\nprint(filtered_sentence)\n\n ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:05:29.193123Z","iopub.execute_input":"2023-11-22T19:05:29.193565Z","iopub.status.idle":"2023-11-22T19:05:29.201233Z","shell.execute_reply.started":"2023-11-22T19:05:29.193527Z","shell.execute_reply":"2023-11-22T19:05:29.199977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stemming ","metadata":{}},{"cell_type":"code","source":"\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK resources (if not already downloaded)\nnltk.download('punkt')\n\n# Create a Porter stemmer\nporter_stemmer = PorterStemmer()\n\n# Replace 'your_text_here' with your actual text\nyour_text_here = \"Walking in the walks is funnier than running to the ran.\"\n\n# Tokenize the text into words\nwords = word_tokenize(your_text_here)\n\n# Apply stemming\nstemmed_words = [porter_stemmer.stem(word) for word in words]\n\n# Join the stemmed words back into a sentence\nstemmed_sentence = ' '.join(stemmed_words)\n\nprint(f'Original Sentence: {your_text_here}')\nprint(f'Stemmed Sentence: {stemmed_sentence}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:36:37.345967Z","iopub.execute_input":"2023-11-22T19:36:37.346434Z","iopub.status.idle":"2023-11-22T19:36:37.476099Z","shell.execute_reply.started":"2023-11-22T19:36:37.346381Z","shell.execute_reply":"2023-11-22T19:36:37.474969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence segmentation","metadata":{}},{"cell_type":"code","source":"text=\"hello! how are you? I hope you are doing well. Have a great day.\"\nsent=nltk.sent_tokenize(text)\nfor sent in sent:\n    print(sent)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:40:47.907103Z","iopub.execute_input":"2023-11-22T19:40:47.907495Z","iopub.status.idle":"2023-11-22T19:40:47.913691Z","shell.execute_reply.started":"2023-11-22T19:40:47.907463Z","shell.execute_reply":"2023-11-22T19:40:47.912548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NER (name entity Recognition)","metadata":{}},{"cell_type":"code","source":"nltk.download('maxent_ne_chunker')","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:44:08.974631Z","iopub.execute_input":"2023-11-22T19:44:08.975039Z","iopub.status.idle":"2023-11-22T19:44:09.057826Z","shell.execute_reply.started":"2023-11-22T19:44:08.975007Z","shell.execute_reply":"2023-11-22T19:44:09.056993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import pos_tag,ne_chunk\n\npos_tags = pos_tag(filtered_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:46:24.617905Z","iopub.execute_input":"2023-11-22T19:46:24.618356Z","iopub.status.idle":"2023-11-22T19:46:24.624867Z","shell.execute_reply.started":"2023-11-22T19:46:24.618320Z","shell.execute_reply":"2023-11-22T19:46:24.623530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_tags","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:46:26.016897Z","iopub.execute_input":"2023-11-22T19:46:26.017310Z","iopub.status.idle":"2023-11-22T19:46:26.025012Z","shell.execute_reply.started":"2023-11-22T19:46:26.017264Z","shell.execute_reply":"2023-11-22T19:46:26.023806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"named_entities=ne_chunk(pos_tags)\nprint(named_entities)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:48:16.437435Z","iopub.execute_input":"2023-11-22T19:48:16.437883Z","iopub.status.idle":"2023-11-22T19:48:16.449092Z","shell.execute_reply.started":"2023-11-22T19:48:16.437845Z","shell.execute_reply":"2023-11-22T19:48:16.447799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiword extraction","metadata":{}},{"cell_type":"code","source":"from nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures","metadata":{"execution":{"iopub.status.busy":"2023-11-22T19:53:39.098140Z","iopub.execute_input":"2023-11-22T19:53:39.098583Z","iopub.status.idle":"2023-11-22T19:53:39.103623Z","shell.execute_reply.started":"2023-11-22T19:53:39.098551Z","shell.execute_reply":"2023-11-22T19:53:39.102682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_1= \"He bought a cup of coffee and sat down at the coffee shop.\"\n\ntokens=nltk.word_tokenize(text_1)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:04:50.100908Z","iopub.execute_input":"2023-11-22T20:04:50.101397Z","iopub.status.idle":"2023-11-22T20:04:50.107858Z","shell.execute_reply.started":"2023-11-22T20:04:50.101362Z","shell.execute_reply":"2023-11-22T20:04:50.106635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the bigram  collocation finder\n\nbigram=BigramAssocMeasures()\nfinder=BigramCollocationFinder.from_words(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:04:52.503425Z","iopub.execute_input":"2023-11-22T20:04:52.504517Z","iopub.status.idle":"2023-11-22T20:04:52.508553Z","shell.execute_reply.started":"2023-11-22T20:04:52.504477Z","shell.execute_reply":"2023-11-22T20:04:52.507743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp_collocation=finder.nbest(bigram.pmi,9)\nfor collocation in tp_collocation:\n    print(\" \".join(collocation))","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:05:04.790968Z","iopub.execute_input":"2023-11-22T20:05:04.791414Z","iopub.status.idle":"2023-11-22T20:05:04.797904Z","shell.execute_reply.started":"2023-11-22T20:05:04.791375Z","shell.execute_reply":"2023-11-22T20:05:04.796615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# 1. Assignment 01: Define a function to implement inflectiona morphology for the given list of words","metadata":{}},{"cell_type":"code","source":"WORDS= [\"WALK\",\"PLAY\",\"JUMP\",\"READ\"]\nWORDS","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:06:41.769249Z","iopub.execute_input":"2023-11-22T20:06:41.769657Z","iopub.status.idle":"2023-11-22T20:06:41.776203Z","shell.execute_reply.started":"2023-11-22T20:06:41.769626Z","shell.execute_reply":"2023-11-22T20:06:41.775413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"inflectional morphology","metadata":{}},{"cell_type":"code","source":"def get_inflectional_morphology(WORDS):\n    word_dict=dict()\n    inflections=[\"s\",\"ed\",\"ing\"]\n    \n    for word in WORDS:\n        morphenes=list()\n        for inflection in inflections:\n            morphenes.append(word + inflection)\n            \n        word_dict[word]= morphenes\n            \n    return word_dict\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:19:16.103247Z","iopub.execute_input":"2023-11-22T20:19:16.103740Z","iopub.status.idle":"2023-11-22T20:19:16.110239Z","shell.execute_reply.started":"2023-11-22T20:19:16.103703Z","shell.execute_reply":"2023-11-22T20:19:16.109429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_inflectional_morphology(WORDS)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:19:28.359850Z","iopub.execute_input":"2023-11-22T20:19:28.360234Z","iopub.status.idle":"2023-11-22T20:19:28.366774Z","shell.execute_reply.started":"2023-11-22T20:19:28.360204Z","shell.execute_reply":"2023-11-22T20:19:28.365977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Derivational Morphology","metadata":{}},{"cell_type":"code","source":"words = [\n\n    (\"teach\", \"-er\"),\n    (\"real\", \"-ize\"),\n]\n\nfor word, affix in words:\n    derived_word = word + affix[1:]\n\n    print(f\"Original Word: {word}\")\n    print(f\"Affix: {affix}\")\n    print(f\"Derived Word: {derived_word}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-22T20:19:52.856125Z","iopub.execute_input":"2023-11-22T20:19:52.856536Z","iopub.status.idle":"2023-11-22T20:19:52.863878Z","shell.execute_reply.started":"2023-11-22T20:19:52.856504Z","shell.execute_reply":"2023-11-22T20:19:52.862724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. implementation of bigram and trigram","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk import bigrams,trigrams\nfrom nltk.tokenize  import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import reuters","metadata":{"execution":{"iopub.status.busy":"2023-11-23T04:54:13.900772Z","iopub.execute_input":"2023-11-23T04:54:13.901301Z","iopub.status.idle":"2023-11-23T04:54:15.286937Z","shell.execute_reply.started":"2023-11-23T04:54:13.901216Z","shell.execute_reply":"2023-11-23T04:54:15.285548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('reuters')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T04:54:15.289079Z","iopub.execute_input":"2023-11-23T04:54:15.290536Z","iopub.status.idle":"2023-11-23T04:54:15.637995Z","shell.execute_reply.started":"2023-11-23T04:54:15.290488Z","shell.execute_reply":"2023-11-23T04:54:15.636993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text= \"your Destination is Kabar do some workship, Galib some said that never go any where without taking somthing\"\n\ntokens = word_tokenize(sample_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T04:54:44.228060Z","iopub.execute_input":"2023-11-23T04:54:44.228451Z","iopub.status.idle":"2023-11-23T04:54:44.245418Z","shell.execute_reply.started":"2023-11-23T04:54:44.228420Z","shell.execute_reply":"2023-11-23T04:54:44.243797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigram=list(bigrams(tokens))\ntrigram=list(trigrams(tokens))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T04:57:27.031270Z","iopub.execute_input":"2023-11-23T04:57:27.031991Z","iopub.status.idle":"2023-11-23T04:57:27.037675Z","shell.execute_reply.started":"2023-11-23T04:57:27.031929Z","shell.execute_reply":"2023-11-23T04:57:27.036409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigram","metadata":{"execution":{"iopub.status.busy":"2023-11-23T04:57:39.783441Z","iopub.execute_input":"2023-11-23T04:57:39.783873Z","iopub.status.idle":"2023-11-23T04:57:39.793285Z","shell.execute_reply.started":"2023-11-23T04:57:39.783839Z","shell.execute_reply":"2023-11-23T04:57:39.791834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trigram","metadata":{"execution":{"iopub.status.busy":"2023-11-23T04:57:46.945367Z","iopub.execute_input":"2023-11-23T04:57:46.945734Z","iopub.status.idle":"2023-11-23T04:57:46.954708Z","shell.execute_reply.started":"2023-11-23T04:57:46.945705Z","shell.execute_reply":"2023-11-23T04:57:46.953357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Frequency=FreqDist(bigram)\nFrequency","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:00:16.163636Z","iopub.execute_input":"2023-11-23T05:00:16.164028Z","iopub.status.idle":"2023-11-23T05:00:16.171168Z","shell.execute_reply.started":"2023-11-23T05:00:16.163997Z","shell.execute_reply":"2023-11-23T05:00:16.169948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fres=FreqDist(trigram)\nFres","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:02:15.849036Z","iopub.execute_input":"2023-11-23T05:02:15.849900Z","iopub.status.idle":"2023-11-23T05:02:15.857299Z","shell.execute_reply.started":"2023-11-23T05:02:15.849864Z","shell.execute_reply":"2023-11-23T05:02:15.855997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# implementation of POS TAGGING","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:10:24.888725Z","iopub.execute_input":"2023-11-23T05:10:24.889109Z","iopub.status.idle":"2023-11-23T05:10:24.897124Z","shell.execute_reply.started":"2023-11-23T05:10:24.889078Z","shell.execute_reply":"2023-11-23T05:10:24.896052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_text=\"Part-of-speech tagging is an important task in natural language processing.\"\ntokenization=word_tokenize(my_text)\npos_tags=pos_tag(tokens)\npos_tags","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:10:45.934160Z","iopub.execute_input":"2023-11-23T05:10:45.934612Z","iopub.status.idle":"2023-11-23T05:10:45.946248Z","shell.execute_reply.started":"2023-11-23T05:10:45.934581Z","shell.execute_reply":"2023-11-23T05:10:45.944793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement chunking to extract noun phrase","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag,ne_chunk\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:14:23.659327Z","iopub.execute_input":"2023-11-23T05:14:23.659703Z","iopub.status.idle":"2023-11-23T05:14:23.668539Z","shell.execute_reply.started":"2023-11-23T05:14:23.659675Z","shell.execute_reply":"2023-11-23T05:14:23.667218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"your_text_here = \"Natural language processing is a fascinating field that involves analyzing and understanding human language  .\"\n\ntokens=word_tokenize(your_text_here)\npos_tg=pos_tag(tokens)\npos_tg","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:19:51.680295Z","iopub.execute_input":"2023-11-23T05:19:51.680729Z","iopub.status.idle":"2023-11-23T05:19:51.692175Z","shell.execute_reply.started":"2023-11-23T05:19:51.680697Z","shell.execute_reply":"2023-11-23T05:19:51.690999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grammar= \"NP: {<DT>?<JJ>*<NN>}\"\nchunk_parser = RegexpParser(grammar)\nchunk_parser","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:31:24.627299Z","iopub.execute_input":"2023-11-23T05:31:24.627684Z","iopub.status.idle":"2023-11-23T05:31:24.636474Z","shell.execute_reply.started":"2023-11-23T05:31:24.627655Z","shell.execute_reply":"2023-11-23T05:31:24.634955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noun_parse=[]\ntree = chunk_parser.parse(pos_tags)\nfor subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n    noun_phrases.append(' '.join(word for word, tag in subtree.leaves()))\n\n# Print the result\nprint(f\"Original Text: {your_text_here}\\n\")\nprint(f\"Noun Phrases: {noun_phrases}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:32:16.872569Z","iopub.execute_input":"2023-11-23T05:32:16.873044Z","iopub.status.idle":"2023-11-23T05:32:16.880898Z","shell.execute_reply.started":"2023-11-23T05:32:16.873010Z","shell.execute_reply":"2023-11-23T05:32:16.879516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement chunking to extract  noun phrase","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag, ne_chunk\nnltk.download('punkt')\n\n# Replace 'your_text_here' with your actual text\nyour_text_here = \"Apple Inc. was founded by Steve Jobs and Steve Wozniak. Its headquarters are in Cupertino, California.\"\n\n# Tokenize the text\ntokens = word_tokenize(your_text_here)\n\n# Perform Part-of-Speech tagging\npos_tags = pos_tag(tokens)\n\n# Perform named entity recognition\nne_chunks = ne_chunk(pos_tags)\n\n# Extract named entities\nnamed_entities = []\nfor chunk in ne_chunks:\n    if hasattr(chunk, 'label'):\n        named_entities.append(' '.join(c[0] for c in chunk.leaves()))\n\n# Print the result\nprint(f\"Original Text: {your_text_here}\\n\")\nprint(f\"Named Entities: {named_entities}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T05:34:55.074500Z","iopub.execute_input":"2023-11-23T05:34:55.074906Z","iopub.status.idle":"2023-11-23T05:34:55.094778Z","shell.execute_reply.started":"2023-11-23T05:34:55.074875Z","shell.execute_reply":"2023-11-23T05:34:55.092942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement for spelling correction","metadata":{}},{"cell_type":"code","source":"pip install pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:44:44.316370Z","iopub.execute_input":"2023-11-23T06:44:44.316990Z","iopub.status.idle":"2023-11-23T06:44:58.794464Z","shell.execute_reply.started":"2023-11-23T06:44:44.316947Z","shell.execute_reply":"2023-11-23T06:44:58.792109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\n\n# Replace 'your_text_here' with your actual text\nyour_text_here = \"Thiss is an example of spellling correctoin.\"\n\n# Tokenize the text into words\nwords = your_text_here.split()\n\n# Initialize SpellChecker\nspell = SpellChecker()\n\n# Find and correct misspelled words\ncorrected_text = []\nfor word in words:\n    corrected_text.append(spell.correction(word))\n\n# Join the corrected words back into a sentence\ncorrected_sentence = ' '.join(corrected_text)\n\n# Print the result\nprint(f\"Original Text: {your_text_here}\\n\")\nprint(f\"Corrected Text: {corrected_sentence}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:44:58.798358Z","iopub.execute_input":"2023-11-23T06:44:58.798995Z","iopub.status.idle":"2023-11-23T06:44:59.440077Z","shell.execute_reply.started":"2023-11-23T06:44:58.798929Z","shell.execute_reply":"2023-11-23T06:44:59.437995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"implement the next word prediction","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk import bigrams, FreqDist, ConditionalFreqDist\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK resources (if not already downloaded)\nnltk.download('punkt')\n\n# Replace 'your_text_here' with your actual text\nyour_text_here = \"This is an example sentence. Another example sentence follows.\"\n\n# Tokenize the text into words\nwords = word_tokenize(your_text_here.lower())  # Convert to lowercase for case-insensitive matching\n\n# Create bigrams\nbi_grams = list(bigrams(words))\n\n# Create conditional frequency distribution\ncfd = ConditionalFreqDist(bi_grams)\n\n# Function to predict the next word\ndef predict_next_word(word):\n    word = word.lower()  # Convert to lowercase for case-insensitive matching\n    next_word_freq = cfd[word]\n    predicted_word = next_word_freq.max()\n    return predicted_word\n\n# Test the prediction\ninput_word = \"example\"\npredicted_next_word = predict_next_word(input_word)\n\n# Print the result\nprint(f\"Input Word: {input_word}\")\nprint(f\"Predicted Next Word: {predicted_next_word}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:46:36.907535Z","iopub.execute_input":"2023-11-23T06:46:36.908045Z","iopub.status.idle":"2023-11-23T06:46:36.921740Z","shell.execute_reply.started":"2023-11-23T06:46:36.908009Z","shell.execute_reply":"2023-11-23T06:46:36.920328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image caption using transformer or LSTM/Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n# Load a pre-trained image captioning model and processor\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# Load and preprocess an image\nimage_path = \"/kaggle/input/data-3/WIN_20221212_20_40_06_Pro.jpg\"\nimage = Image.open(image_path)\ninputs = processor(images=image, return_tensors=\"pt\")\n\n# Generate image caption\nwith torch.no_grad():\n    caption_ids = model.generate(**inputs)\ncaption = processor.decode(caption_ids[0], skip_special_tokens=True)\n\n# Print the result\nprint(f\"Image Caption: {caption}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:52:56.378409Z","iopub.execute_input":"2023-11-23T06:52:56.378805Z","iopub.status.idle":"2023-11-23T06:53:43.648664Z","shell.execute_reply.started":"2023-11-23T06:52:56.378776Z","shell.execute_reply":"2023-11-23T06:53:43.646947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_df[\"sentence\"][15]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:59:15.438920Z","iopub.execute_input":"2023-11-23T06:59:15.439500Z","iopub.status.idle":"2023-11-23T06:59:15.451614Z","shell.execute_reply.started":"2023-11-23T06:59:15.439455Z","shell.execute_reply":"2023-11-23T06:59:15.449954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# text generation\n","metadata":{}},{"cell_type":"code","source":"import random\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport re\nimport spacy\nimport nltk\nfrom nltk.corpus import wordnet\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import LSTM, Dense, Activation\nfrom tensorflow.keras.optimizers import RMSprop, Adam , SGD\nnltk.download('punkt')\nnltk.download('wordnet')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:58:07.135196Z","iopub.execute_input":"2023-11-23T06:58:07.135740Z","iopub.status.idle":"2023-11-23T06:58:09.104233Z","shell.execute_reply.started":"2023-11-23T06:58:07.135698Z","shell.execute_reply":"2023-11-23T06:58:09.102837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_df=pd.read_csv(\"/kaggle/input/private/tablesWithTag.csv\")\ntext_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:58:39.169650Z","iopub.execute_input":"2023-11-23T06:58:39.171059Z","iopub.status.idle":"2023-11-23T06:58:40.634168Z","shell.execute_reply.started":"2023-11-23T06:58:39.171005Z","shell.execute_reply":"2023-11-23T06:58:40.632579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_df[\"sentence\"][15]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:59:33.128641Z","iopub.execute_input":"2023-11-23T06:59:33.129321Z","iopub.status.idle":"2023-11-23T06:59:33.143900Z","shell.execute_reply.started":"2023-11-23T06:59:33.129237Z","shell.execute_reply":"2023-11-23T06:59:33.139444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(text_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:59:42.456337Z","iopub.execute_input":"2023-11-23T06:59:42.457490Z","iopub.status.idle":"2023-11-23T06:59:42.470074Z","shell.execute_reply.started":"2023-11-23T06:59:42.457444Z","shell.execute_reply":"2023-11-23T06:59:42.468392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_df[\"sentence\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T06:59:51.819185Z","iopub.execute_input":"2023-11-23T06:59:51.820807Z","iopub.status.idle":"2023-11-23T06:59:51.835961Z","shell.execute_reply.started":"2023-11-23T06:59:51.820722Z","shell.execute_reply":"2023-11-23T06:59:51.833923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt=list(text_df.sentence.values)\njoined_txt=\" \".join(txt)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:00:00.597414Z","iopub.execute_input":"2023-11-23T07:00:00.598401Z","iopub.status.idle":"2023-11-23T07:00:00.631756Z","shell.execute_reply.started":"2023-11-23T07:00:00.598341Z","shell.execute_reply":"2023-11-23T07:00:00.629925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"partial_txt=joined_txt[:100000]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:00:06.451403Z","iopub.execute_input":"2023-11-23T07:00:06.453038Z","iopub.status.idle":"2023-11-23T07:00:06.462152Z","shell.execute_reply.started":"2023-11-23T07:00:06.452967Z","shell.execute_reply":"2023-11-23T07:00:06.459725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"partial_txt = re.sub(r'[^\\w\\s]', '', partial_txt)\npartial_txt=partial_txt.lower()\ntokens=nltk.word_tokenize(partial_txt)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:00:17.512093Z","iopub.execute_input":"2023-11-23T07:00:17.512967Z","iopub.status.idle":"2023-11-23T07:00:17.596964Z","shell.execute_reply.started":"2023-11-23T07:00:17.512929Z","shell.execute_reply":"2023-11-23T07:00:17.594794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:00:25.184356Z","iopub.execute_input":"2023-11-23T07:00:25.185878Z","iopub.status.idle":"2023-11-23T07:00:25.213471Z","shell.execute_reply.started":"2023-11-23T07:00:25.185811Z","shell.execute_reply":"2023-11-23T07:00:25.212207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_tokens=np.unique(tokens)\nunique_token_index={token: idx for idx,token in enumerate(unique_tokens)}","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:00:50.284594Z","iopub.execute_input":"2023-11-23T07:00:50.285749Z","iopub.status.idle":"2023-11-23T07:00:50.311003Z","shell.execute_reply.started":"2023-11-23T07:00:50.285602Z","shell.execute_reply":"2023-11-23T07:00:50.308576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_token_index","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:00:56.719624Z","iopub.execute_input":"2023-11-23T07:00:56.720529Z","iopub.status.idle":"2023-11-23T07:00:56.760025Z","shell.execute_reply.started":"2023-11-23T07:00:56.720468Z","shell.execute_reply":"2023-11-23T07:00:56.758035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_words =10\ninput_words =[]\nnxt_words=[]\n\nfor i in range(len(tokens)- n_words):\n    input_words.append(tokens[i:i + n_words])\n    nxt_words.append(tokens[i + n_words])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:01:11.360558Z","iopub.execute_input":"2023-11-23T07:01:11.362387Z","iopub.status.idle":"2023-11-23T07:01:11.394665Z","shell.execute_reply.started":"2023-11-23T07:01:11.362243Z","shell.execute_reply":"2023-11-23T07:01:11.392551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_words","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:02:10.791425Z","iopub.execute_input":"2023-11-23T07:02:10.791961Z","iopub.status.idle":"2023-11-23T07:02:11.004894Z","shell.execute_reply.started":"2023-11-23T07:02:10.791909Z","shell.execute_reply":"2023-11-23T07:02:11.003679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nxt_words","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:02:30.246414Z","iopub.execute_input":"2023-11-23T07:02:30.246948Z","iopub.status.idle":"2023-11-23T07:02:30.274168Z","shell.execute_reply.started":"2023-11-23T07:02:30.246906Z","shell.execute_reply":"2023-11-23T07:02:30.272838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=np.zeros((len(input_words),n_words,len(unique_tokens)),dtype=bool)\ny=np.zeros((len(nxt_words),len(unique_tokens)),dtype=bool)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:02:46.760110Z","iopub.execute_input":"2023-11-23T07:02:46.760556Z","iopub.status.idle":"2023-11-23T07:02:46.782245Z","shell.execute_reply.started":"2023-11-23T07:02:46.760523Z","shell.execute_reply":"2023-11-23T07:02:46.780751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, words in enumerate(input_words):\n    for j, word in enumerate(words):\n        x[i,j, unique_token_index[word]] =1\n        y[i, unique_token_index[nxt_words[i]]] =1","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:02:53.640950Z","iopub.execute_input":"2023-11-23T07:02:53.641427Z","iopub.status.idle":"2023-11-23T07:02:54.528933Z","shell.execute_reply.started":"2023-11-23T07:02:53.641394Z","shell.execute_reply":"2023-11-23T07:02:54.527569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\nmodel.add(LSTM(128, input_shape=(n_words,len(unique_tokens)),return_sequences=True))\nmodel.add(LSTM(128))\nmodel.add(Dense(len(unique_tokens)))\nmodel.add(Activation(\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:03:03.056115Z","iopub.execute_input":"2023-11-23T07:03:03.056712Z","iopub.status.idle":"2023-11-23T07:03:03.817784Z","shell.execute_reply.started":"2023-11-23T07:03:03.056660Z","shell.execute_reply":"2023-11-23T07:03:03.816751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\",optimizer=RMSprop(learning_rate=0.01),metrics=[\"accuracy\"])\nmodel.fit(x,y,batch_size=128,epochs=50,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:04:13.161863Z","iopub.execute_input":"2023-11-23T07:04:13.163576Z","iopub.status.idle":"2023-11-23T07:27:36.814197Z","shell.execute_reply.started":"2023-11-23T07:04:13.163522Z","shell.execute_reply":"2023-11-23T07:27:36.813118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_next_word(input_txt,n_best):\n    input_txt = input_txt.lower()\n    x=np.zeros((1,n_words,len(unique_tokens)))\n    for i, word in enumerate(input_txt.split()):\n        x[0,i,unique_token_index[word]]=1\n    predictions=model.predict(x)[0]\n    return np.argpartition(predictions,-n_best)[-n_best:]","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:08.801176Z","iopub.execute_input":"2023-11-23T07:38:08.801685Z","iopub.status.idle":"2023-11-23T07:38:08.808958Z","shell.execute_reply.started":"2023-11-23T07:38:08.801652Z","shell.execute_reply":"2023-11-23T07:38:08.807554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"possible=predict_next_word(\"The Chicago Bears recent first round selection was\",5)\npossible","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:10.160734Z","iopub.execute_input":"2023-11-23T07:38:10.161909Z","iopub.status.idle":"2023-11-23T07:38:11.267017Z","shell.execute_reply.started":"2023-11-23T07:38:10.161868Z","shell.execute_reply":"2023-11-23T07:38:11.265759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([unique_tokens[idx] for idx in possible])","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:11.269742Z","iopub.execute_input":"2023-11-23T07:38:11.270570Z","iopub.status.idle":"2023-11-23T07:38:11.277292Z","shell.execute_reply.started":"2023-11-23T07:38:11.270531Z","shell.execute_reply":"2023-11-23T07:38:11.275602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_text(input_txt, text_length,creativity=3):\n    word_seq=input_txt.split()\n    current=0\n    for _ in range(text_length):\n        sub_seq=\" \".join(nltk.word_tokenize(\" \".join(word_seq).lower())[current:current+n_words])\n        try:\n            choice = unique_tokens[random.choice(predict_next_word(sub_seq,creativity))]\n        except:\n            choice = random.choice(unique_tokens)\n        word_seq.append(choice)\n        current +=1\n    return \" \".join(word_seq)    ","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:11.280143Z","iopub.execute_input":"2023-11-23T07:38:11.280730Z","iopub.status.idle":"2023-11-23T07:38:11.289738Z","shell.execute_reply.started":"2023-11-23T07:38:11.280693Z","shell.execute_reply":"2023-11-23T07:38:11.288509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_text(\"Who scored the most in the game \",100,2)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:11.292908Z","iopub.execute_input":"2023-11-23T07:38:11.293358Z","iopub.status.idle":"2023-11-23T07:38:21.701821Z","shell.execute_reply.started":"2023-11-23T07:38:11.293319Z","shell.execute_reply":"2023-11-23T07:38:21.700321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')  # Download the tokenizer models\nnltk.download('wordnet')\nimport spacy\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.704923Z","iopub.execute_input":"2023-11-23T07:38:21.705723Z","iopub.status.idle":"2023-11-23T07:38:21.716651Z","shell.execute_reply.started":"2023-11-23T07:38:21.705654Z","shell.execute_reply":"2023-11-23T07:38:21.715187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df=pd.read_csv(\"/kaggle/input/traindata/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.722937Z","iopub.execute_input":"2023-11-23T07:38:21.724585Z","iopub.status.idle":"2023-11-23T07:38:21.735121Z","shell.execute_reply.started":"2023-11-23T07:38:21.724508Z","shell.execute_reply":"2023-11-23T07:38:21.732202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.737371Z","iopub.execute_input":"2023-11-23T07:38:21.737905Z","iopub.status.idle":"2023-11-23T07:38:21.807814Z","shell.execute_reply.started":"2023-11-23T07:38:21.737861Z","shell.execute_reply":"2023-11-23T07:38:21.804524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.809124Z","iopub.status.idle":"2023-11-23T07:38:21.810226Z","shell.execute_reply.started":"2023-11-23T07:38:21.809953Z","shell.execute_reply":"2023-11-23T07:38:21.809984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_numbers(list_text):\n    list_text_new=[]\n    for i in list_text:\n        if not re.search('\\d',i):\n            list_text_new.append(i)\n    return ''.join(list_text_new)\ndf['text']=df['text'].apply(drop_numbers)\n   ","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.812116Z","iopub.status.idle":"2023-11-23T07:38:21.812647Z","shell.execute_reply.started":"2023-11-23T07:38:21.812420Z","shell.execute_reply":"2023-11-23T07:38:21.812443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.814289Z","iopub.status.idle":"2023-11-23T07:38:21.814877Z","shell.execute_reply.started":"2023-11-23T07:38:21.814634Z","shell.execute_reply":"2023-11-23T07:38:21.814659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lower_case(text):\n    text_words=word_tokenize(text)\n    text_words_lower=[x.lower() for x in text_words]\n    return ' '.join(text_words_lower)\ndf['text']=df['text'].apply(lower_case)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.816872Z","iopub.status.idle":"2023-11-23T07:38:21.817444Z","shell.execute_reply.started":"2023-11-23T07:38:21.817139Z","shell.execute_reply":"2023-11-23T07:38:21.817160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\n#defining a function for lemmitization \ndef lemmatize_text(text):\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_ for token in doc]\n    lemmatized_text = ' '.join(lemmatized_tokens)\n    return lemmatized_text\ndf['text']=df['text'].apply(lemmatize_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.819867Z","iopub.status.idle":"2023-11-23T07:38:21.820439Z","shell.execute_reply.started":"2023-11-23T07:38:21.820169Z","shell.execute_reply":"2023-11-23T07:38:21.820192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(text):\n    text_tokens =word_tokenize(text)\n    tokens =[word for word in text_tokens if not word in set(stopwords.words('english'))]\n    tokens_text = ' '.join(tokens)\n    return tokens_text\ndf['text']=df['text'].apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.831739Z","iopub.status.idle":"2023-11-23T07:38:21.832434Z","shell.execute_reply.started":"2023-11-23T07:38:21.832089Z","shell.execute_reply":"2023-11-23T07:38:21.832111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef remove_punctuation(text):\n    pattern = r'[^\\w\\s]'\n    text_no_punct = re.sub(pattern, '', text)\n    return text_no_punct\n\ndf[\"text\"] = df['text'].apply(remove_punctuation)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.834573Z","iopub.status.idle":"2023-11-23T07:38:21.835131Z","shell.execute_reply.started":"2023-11-23T07:38:21.834880Z","shell.execute_reply":"2023-11-23T07:38:21.834903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets calculate the Polarity of the Reviews\ndef get_polarity(text):\n    textblob = TextBlob(str(text))\n    pol = textblob.sentiment.polarity\n    if(pol==0):\n        return \"Neutral\"\n    elif(pol>0 and pol<=0.3):\n        return \"Weakly Positive\"\n    elif(pol>0.3 and pol<=0.6):\n        return \"Positive\"\n    elif(pol>0.6 and pol<=1):\n        return \"Strongly Positive\"\n    elif(pol>-0.3 and pol<=0):\n        return \"Weakly Negative\"\n    elif(pol>-0.6 and pol<=-0.3):\n        return \"Negative\"\n    elif(pol>-1 and pol<=-0.6):\n        return \"Strongly Negative\"\n    \ndf['polarity'] = df['text'].apply(get_polarity)","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.837127Z","iopub.status.idle":"2023-11-23T07:38:21.837807Z","shell.execute_reply.started":"2023-11-23T07:38:21.837534Z","shell.execute_reply":"2023-11-23T07:38:21.837559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['polarity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.839919Z","iopub.status.idle":"2023-11-23T07:38:21.840498Z","shell.execute_reply.started":"2023-11-23T07:38:21.840217Z","shell.execute_reply":"2023-11-23T07:38:21.840240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.841828Z","iopub.status.idle":"2023-11-23T07:38:21.842381Z","shell.execute_reply.started":"2023-11-23T07:38:21.842118Z","shell.execute_reply":"2023-11-23T07:38:21.842139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neutral = 0\nwpositive = 0\nspositive = 0\npositive = 0\nnegative = 0\nwnegative = 0\nsnegative = 0\npolarity = 0\n\nfor i in range(0,len(df)):\n    textblob = TextBlob(str(df['text'][i]))\n    polarity+= textblob.sentiment.polarity\n    pol = textblob.sentiment.polarity\n    if (pol == 0):  # adding reaction of how people are reacting to find average later\n        neutral += 1\n    elif (pol > 0 and pol <= 0.3):\n        wpositive += 1\n    elif (pol > 0.3 and pol <= 0.6):\n        positive += 1\n    elif (pol > 0.6 and pol <= 1):\n        spositive += 1\n    elif (pol > -0.3 and pol <= 0):\n        wnegative += 1\n    elif (pol > -0.6 and pol <= -0.3):\n        negative += 1\n    elif (pol > -1 and pol <= -0.6):\n        snegative += 1\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.844777Z","iopub.status.idle":"2023-11-23T07:38:21.845611Z","shell.execute_reply.started":"2023-11-23T07:38:21.845196Z","shell.execute_reply":"2023-11-23T07:38:21.845235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding average reaction\npolarity = polarity / len(df)\npolarity","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.849818Z","iopub.status.idle":"2023-11-23T07:38:21.850488Z","shell.execute_reply.started":"2023-11-23T07:38:21.850179Z","shell.execute_reply":"2023-11-23T07:38:21.850209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def percentage(part, whole):\n    temp = 100 * float(part) / float(whole)\n    return format(temp, '.2f')","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.853043Z","iopub.status.idle":"2023-11-23T07:38:21.853705Z","shell.execute_reply.started":"2023-11-23T07:38:21.853401Z","shell.execute_reply":"2023-11-23T07:38:21.853423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding average of how people are reacting\nNoOfTerms=len(df)\npositive = percentage(positive, NoOfTerms)\nwpositive = percentage(wpositive, NoOfTerms)\nspositive = percentage(spositive, NoOfTerms)\nnegative = percentage(negative, NoOfTerms)\nwnegative = percentage(wnegative, NoOfTerms)\nsnegative = percentage(snegative, NoOfTerms)\nneutral = percentage(neutral, NoOfTerms)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.856324Z","iopub.status.idle":"2023-11-23T07:38:21.856946Z","shell.execute_reply.started":"2023-11-23T07:38:21.856686Z","shell.execute_reply":"2023-11-23T07:38:21.856711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # printing out data\nsearchTerm='shahrukh khan'\nprint(\"How people are reacting on \" + searchTerm + \" by analyzing \" + str(NoOfTerms) + \" tweets.\")\nprint()\nprint(\"-----------------------------------------------------------------------------------------\")\nprint()\nprint(\"General Report: \")\n\nif (polarity == 0):\n    print(\"Neutral\")\nelif (polarity > 0 and polarity <= 0.3):\n    print(\"Weakly Positive\")\nelif (polarity > 0.3 and polarity <= 0.6):\n    print(\"Positive\")\nelif (polarity > 0.6 and polarity <= 1):\n    print(\"Strongly Positive\")\nelif (polarity > -0.3 and polarity <= 0):\n    print(\"Weakly Negative\")\nelif (polarity > -0.6 and polarity <= -0.3):\n    print(\"Negative\")\nelif (polarity > -1 and polarity <= -0.6):\n    print(\"Strongly Negative\")\n\nprint()\nprint(\"------------------------------------------------------------------------------------------\")\nprint()\nprint(\"Detailed Report: \")\nprint(str(positive) + \"% people thought it was positive\")\nprint(str(wpositive) + \"% people thought it was weakly positive\")\nprint(str(spositive) + \"% people thought it was strongly positive\")\nprint(str(negative) + \"% people thought it was negative\")\nprint(str(wnegative) + \"% people thought it was weakly negative\")\nprint(str(snegative) + \"% people thought it was strongly negative\")\nprint(str(neutral) + \"% people thought it was neutral\")","metadata":{"execution":{"iopub.status.busy":"2023-11-23T07:38:21.858962Z","iopub.status.idle":"2023-11-23T07:38:21.859621Z","shell.execute_reply.started":"2023-11-23T07:38:21.859349Z","shell.execute_reply":"2023-11-23T07:38:21.859377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sizes = [positive, wpositive, spositive, neutral, negative, wnegative, snegative]\ncolors = ['yellowgreen','lightgreen','darkgreen', 'gold', 'red','lightsalmon','darkred']\nlabels = ['Positive [' + str(positive) + '%]', 'Weakly Positive [' + str(wpositive) + '%]',\n          'Strongly Positive [' + str(spositive) + '%]', 'Neutral [' + str(neutral) + '%]',\n          'Negative [' + str(negative) + '%]', 'Weakly Negative [' + str(wnegative) + '%]', \n          'Strongly Negative [' + str(snegative) + '%]']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(sizes, labels = labels, colors = colors)\nplt.legend(labels, loc=\"best\")\nplt.title('How people are reacting on ' + searchTerm + ' by analyzing ' + str(NoOfTerms) + ' Tweets.')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}